# **《机器学习导论》实验报告**

[TOC]



# **编程作业：聚类**

## 实验原理：

参考：

[机器学习实战 (豆瓣) (douban.com)](https://book.douban.com/subject/24703171/)

[(13条消息) 机器学习实战 第十章 利用K-均值聚类算法对未标注数据分组_无名的博客-CSDN博客](https://blog.csdn.net/namelessml/article/details/52694680)



### k-means聚类：

k-means聚类将相似的对象归到同一个簇中，每个簇的中心采用簇中所含值的均值计算而成。

优点：容易实现

缺点：可能收敛到局部最小值，在大规模数据上收敛较慢

适用数据类型：数值型数据

算法流程：

```
    创建k个点作为起始质心（随机选择）
    当任意一个点的簇分配结果发生改变时：
        对数据集中的每个数据点：
         对每个质心：
           计算数据点到质心的距离
       将数据点分配到距其最近的簇
        对每一个簇重新计算其均值作为质心
```



### 二分k-means聚类：

　　k-means的缺点是可能收敛于局部最优解，使用二分k-means聚类算法来解决这个问题。

　　可以使用SSE(sum of squared error误差平方和)来度量聚类的效果，SSE值越小表示数据点接近于它的质心，聚类效果越好。因为对误差取了平方，因此更加重视那些远离质心的点。

　　二分k-means聚类算法的思想是先将所有数据点当作一个簇，然后将该簇一分为二。之后在现有的簇中选择一个簇进行划分，选择哪个簇取决于划分哪个簇后能使SSE值最小。不断重复上述过程，直到达到用户要求的簇的个数。

伪代码：

```
        将所有数据点都看成一个簇
        当簇的数目小于k时：
            初始化lowestSSE = inf
            对于每一个簇：
                对该簇进行k-means聚类(k=2)
                计算聚类后的总误差
                如果小于lowestSSE,则保存聚类后的参数，更新lowestSEE
            选择划分后使得误差值最小的那个簇进行划分
```



### 使用后处理来提高聚类性能：

在包含簇分配结果的矩阵中保存着每个点的误差，即该点到簇质心的距离平方值，可以利用该误差来评价聚类质量的方法。K均值算法有时虽收敛但聚类效果却较差。原因是，K均值算法收敛到了局部最小值，而非全局最小值。

一种度量聚类效果的指标是SSE（Sum of Square Error，误差平方和），对应上面程序中clusterAssment矩阵的第一列之和。SSE值越小表示数据点越接近它们的质心，聚类效果越好。因为对误差取了平方，因此，更加重视那些远离中心的点。增加簇的个数肯定可以降低SSE值，但这违背了聚类的目标。聚类的目标是在保持簇的数目不变的情况下提高簇的质量。

对生成的簇进行后处理，将具有最大SSE值得簇过滤出来，在这些点上运行k-均值算法（其中k=2）划分成两个簇。为保持簇总数不变，可将某两个簇进行合并。两种可量化的合并方法：

合并最近的质心，通过计算所有质心的之间的距离，然后合并距离最近的两个点实现；
合并两个使SSE增幅最小的质心，需要合并两个簇后计算总SSE值。必须在所有可能的两个簇上重复上述过程，直到找到合并最佳的两个簇为止。



## 任务一：对地理数据应用二分k-均值算法聚类

### （1）问题描述：

你的朋友Drew希望你带他去城里庆祝他的生日。由于其他一些朋友也会过来，所以需要你提供一个大家都可行的计划。Drew给了你希望去的69个地址和相应的经纬度。你要决定将这些地方进行聚类的最佳策略，这样可以安排交通工具抵达这些簇的质心，然后步行到每个簇内地址。



### （2）具体实现：

1. 读入数据：

   ```python
   datList = []
   #导入数据
   for line in open('places.txt').readlines():
       lineArr = line.split('\t')
       datList.append([float(lineArr[4]), float(lineArr[3])])
   datMat = mat(datList)
   ```

   

2. 计算两个向量之间的距离：

   ```python
   def distEclud(vecA, vecB):
       return sqrt(sum(power(vecA - vecB, 2)))
   ```

   

3. 随机初始化聚簇中心：

   ```python
   #随机生成簇中心函数
   def randCent(dataSet, k):
       n = shape(dataSet)[1]
       centroids = mat(zeros((k,n)))
       for j in range(n):
           minJ = min(dataSet[:,j]) 
           rangeJ = float(max(dataSet[:,j]) - minJ)
           centroids[:,j] = mat(minJ + rangeJ * random.rand(k,1))
       return centroids
   ```

   

4. 球面距离计算：

   ```python
   #根据经纬度计算球面距离，vecA[0,：]表示A点经纬度
   def distSLC(vecA, vecB):
       a = sin(vecA[0,1]*pi/180) * sin(vecB[0,1]*pi/180)
       b = cos(vecA[0,1]*pi/180) * cos(vecB[0,1]*pi/180) * \
                         cos(pi * (vecB[0,0]-vecA[0,0]) /180)
       return arccos(a + b)*6371.0 
   ```

   

5. **k-means聚类**:

   K-均值是发现给定数据集的k个簇的算法。簇个数k时用户给定的，每个簇通过其质心（centroid），即簇中所有点的中心来描述。

   ```python
   #Kmeans
   def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):
       m = shape(dataSet)[0]  #样本数
       clusterAssment = mat(zeros((m,2))) #获得m*2矩阵（一列簇分类结果，一列误差）
       centroids = createCent(dataSet,k)  #初始化K个质心
       clusterChanged = True #簇更改标记
       while clusterChanged:
           clusterChanged = False
           #样本点加入到最近的簇
           for i in range(m):
               minDist = inf;
               minIndex = -1
               for j in range(k):
                   distJI = distMeas(centroids[j, :], dataSet[i, :])
                   if distJI < minDist:
                       minDist = distJI; minIndex = j
               #该样本划分到距离最近的簇
               if clusterAssment[i,0] != minIndex: clusterChanged = True
               clusterAssment[i,:] = minIndex,minDist**2
               #每轮结束后调整簇心
               for cent in range(k):
                   if clusterAssment[i, 0] != minIndex: clusterChanged = True
                   clusterAssment[i, :] = minIndex, minDist ** 2 #计算每列平均值
           return centroids, clusterAssment
   ```

   

6. **二分k-means聚类**:

   二分K-均值算法（bisecting K-means），它为了克服k-均值算法收敛于局部最小值的问题。该算法首先将所有点作为一个簇，然后将该簇一分为二。之后选择一个簇继续划分，选择哪个簇进行划分取决于对其划分是否可以最大程度降低SSE的值。上述基于SSE的划分过程不断重复，直到得到指定的簇数目为止。

   代码如下：

   ```python
   # k:簇个数   distMeas：距离生成器
   def biKmeans(dataSet, k, distMeas=distEclud):
       m = shape(dataSet)[0]  #数据集矩阵的行数
       clusterAssment = mat(zeros((m,2)))
       centroid0 = mean(dataSet, axis=0).tolist()[0]   # 创建一个初始簇
       centList =[centroid0] #create a list with one centroid
       for j in range(m):  # 计算每个样本点到初始簇的距离
           clusterAssment[j,1] = distMeas(mat(centroid0), dataSet[j,:])**2
       while (len(centList) < k):  # 迭代直到簇的数目等于k
           lowestSSE = inf
           for i in range(len(centList)):
               # 尝试划分每一簇
               ptsInCurrCluster = dataSet[nonzero(clusterAssment[:,0].A==i)[0],:]
               centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)
               sseSplit = sum(splitClustAss[:,1])
               # 剩余数据集的误差
               sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:,0].A!=i)[0],1])
               # 记录总误差最小的那个簇
               if (sseSplit + sseNotSplit) < lowestSSE:
                   bestCentToSplit = i
                   bestNewCents = centroidMat
                   bestClustAss = splitClustAss.copy()
                   lowestSSE = sseSplit + sseNotSplit
           # 因为二分均值，所以结果簇编号只能是0或1，修改结果簇编号为：原簇号和新增簇号
           bestClustAss[nonzero(bestClustAss[:,0].A == 1)[0],0] = len(centList)
           bestClustAss[nonzero(bestClustAss[:,0].A == 0)[0],0] = bestCentToSplit
           # 更新簇列表centlist和样本点分配簇结果矩阵clusterAssment
           centList[bestCentToSplit] = bestNewCents[0,:].tolist()[0]
           centList.append(bestNewCents[1,:].tolist()[0])
           clusterAssment[nonzero(clusterAssment[:,0].A == bestCentToSplit)[0],:]= bestClustAss
       return mat(centList), clusterAssment;
   
   ```

   

### （3）实验结果：

1. k-means聚类：

   ```python
   from matplotlib import pyplot as plt
   import Kmeansch10 as km
   
   data_X = km.loadDataSet("places.txt")
   centroids, clusterAssment = km.kMeans(data_X, 4)
   
   plt.figure()
   plt.scatter(data_X[:, 0].flatten().tolist(), data_X[:, 1].flatten().tolist(), c="b", marker="o")
   plt.scatter(centroids[:, 0].flatten().tolist(), centroids[:, 1].flatten().tolist(), c='r', marker="+")
   plt.show()
   ```

   发现，在经过多次测试后，会出现聚簇收敛到局部最小值，如下所示：

   

   ![image-20210513170800404](https://i.loli.net/2021/05/13/48RxpTDlWyedszf.png)

   

2. 二分k-means聚类：

   k-means的缺点是可能收敛于局部最优解，使用二分k-means聚类算法来解决这个问题。

   ```python
   def clusterClubs(numClust=5):
       datList = []
       #导入数据
       for line in open('places.txt').readlines():
           lineArr = line.split('\t')
           datList.append([float(lineArr[4]), float(lineArr[3])])
       datMat = mat(datList)
       #采用二分k-均值算法进行聚类
       myCentroids, clustAssing = biKmeans(datMat, numClust, distMeas=distSLC)
       #定义画布，背景
       fig = plt.figure()
       rect=[0.0,0.0,1.0,1.0]
       #不同图形标识
       scatterMarkers=['s', 'o', '^', '8', 'p', \
                       'd', 'v', 'h', '>', '<']
       axprops = dict(xticks=[], yticks=[])
       ax0=fig.add_axes(rect, label='ax0', **axprops)
       #导入地图
       imgP = plt.imread('Portland.png')
       ax0.imshow(imgP)
       ax1=fig.add_axes(rect, label='ax1', frameon=False)
       #采用不同图形标识不同簇
       for i in range(numClust):
           ptsInCurrCluster = datMat[nonzero(clustAssing[:,0].A==i)[0],:]
           markerStyle = scatterMarkers[i % len(scatterMarkers)]
           ax1.scatter(ptsInCurrCluster[:,0].flatten().A[0], ptsInCurrCluster[:,1].flatten().A[0], marker=markerStyle, s=90)
       #采用‘+’表示簇中心
       ax1.scatter(myCentroids[:,0].flatten().A[0], myCentroids[:,1].flatten().A[0], marker='+', s=300)
       plt.show()
   ```

   结果如下：

   - K=2时：

     ![image-20210514144309979](https://i.loli.net/2021/05/14/Xo3cCPRDL19xfbQ.png)

     <img src="https://i.loli.net/2021/05/13/RdrxCUs432izQTY.png" alt="image-20210513173814528" style="zoom:80%;" />

     

   - K=3时：

     ![image-20210514144450362](https://i.loli.net/2021/05/14/PkLvs8Xow6UFlMx.png)

     <img src="https://i.loli.net/2021/05/13/Zp1LgEtfOxvrINi.png" alt="image-20210513173914222" style="zoom:80%;" />

     

   - K=4时：

     ![image-20210514144525014](https://i.loli.net/2021/05/14/5zRqxeALGl2QJcO.png)

     <img src="https://i.loli.net/2021/05/13/V7YujaB3lbRdfeh.png" alt="image-20210513174112256" style="zoom:80%;" />

     
   
   - K=5时：
   
     ![image-20210514144610518](https://i.loli.net/2021/05/14/VgzJFTEZfY2PUhk.png)
     
     <img src="https://i.loli.net/2021/05/13/V7YujaB3lbRdfeh.png" alt="image-20210513174201986" style="zoom:80%;" />
   
   





## 任务二：根据用户采集的WiFi信息对用户进行聚类

### （1）数据集讲解

**数据集：**数据集存于DataSetKMeans1.csv与DataSetKMeans2.csv中，两个数据集相互独立。

- **BSSIDLabel:：**SSID标识符，每个AP（接入点，如路由器）拥有1个或多个不同的BSSID，但1个BSSID只属于1个AP；
- **RSSLabel：**该BSSID的信号强度，单位dbm；
- **RoomLabel:** 该BSSID被采集时所属的房间号；
- **SSIDLabel:** 该BSSID的名称，不唯一；
- **finLabel**：finLabel标号相同，表示这部分BSSID在同一时刻被采集到；

我们将在同一时刻采集的所有BSSID及其相应RSS构成的矢量称为一个指纹：
$$
f_i=[〖BSSID〗_1: 〖RSS〗_1,〖BSSID〗_2: 〖RSS〗_2,…]
$$


由于BSSID的RSS在不同位置大小不同，因此指纹可以唯一的标识一个位置。

![image-20210517142727475](https://i.loli.net/2021/05/17/SeOEQz1iU6tMsoF.png)



### （2）实验要求：

编写代码分别对DataSetMeans1.csv和DataSetMeans2.csv两个数据集完成聚类实验，k（k>=2）取不同的值，评估聚类的内部指标DB指数，DB指数定义如下：

DB指数（Davies-Bouldin Index，简称DBI）
$$
\mathrm{DBI}=\frac{1}{k} \sum_{i=1}^{k} \max _{j \neq i}\left(\frac{\operatorname{avg}\left(C_{i}\right)+\operatorname{avg}\left(C_{j}\right)}{d_{\operatorname{cen}}\left(\mu_{i}, \mu_{j}\right)}\right)
$$

其中， ${d_{\operatorname{cen}}\left(\mu_{i}, \mu_{j}\right)}$ 对应于中心点间的距离。显然，DBI的值越小越好。



### （3）具体实现：

1. 数据处理：

   选择有用的数据项['finLabel', 'BSSIDLabel', 'RoomLabel']， 对缺失值，填入-100 。

   一个指纹为一个样本：

   $f_{1}=\left[B S S I D_{1}: RSS_{1}, B S S I D_{2}: RSS_{2}, B S S I D_{3}: RSS_{3}, B S S I D_{4}: RSS_{4}\right]  $

   将数据按 'finLabel' 聚合。 采用所有样本 BSSID 集合的并集作为特征，如指纹 $f_{i}$ 的 BSSID 集合为
   $B_{i}=\left\{B S S I D_{j} \mid B S S I D_{j} \in f_{i}\right\}_{\circ}$
   并按照;
   $$
   \begin{array}{l}
   f_{1}=\left[B S S I D_{1}: -30, B S S I D_{2}: 0, B S S I D_{3}: -45, B S S I D_{4}: -70\right] \\
   f_{2}=\left[B S S I D_{1}: -40, B S S I D_{2}: -80, B S S I D_{3}: -35, B S S I D_{4}: 0\right]
   \end{array}
   $$
   将输入转为向量。

   ```python
   # 预处理
   def loaddata(filename):
       data_f = pd.read_csv(filename, encoding='gbk')
       imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-100)
       data = pd.DataFrame(imputer.fit_transform(data_f))
       data.columns = data_f.columns
       data_feature = data[['finLabel', 'BSSIDLabel', 'RSSLabel']]
       BSSID_v = list(set(data_f['BSSIDLabel']))
       BSSID_l = len(BSSID_v)
       data_bssid = data_feature.groupby('finLabel')
       data_input = []
       for i, v in data_bssid:#每个v仍为df
           tmp = np.array(v['BSSIDLabel'])
           rssdict = dict(zip(v['BSSIDLabel'], v['RSSLabel']))
           tmpa = BSSID_l * [0]
           for bssidv in BSSID_v:
               if bssidv in tmp:
                   tmpa[BSSID_v.index(bssidv)] = rssdict[bssidv]
           data_input.append(tmpa)
       return data_input
       
   ```

   

   

2. 模型构建：

   使用sklearn中的K-means方法，其中，我们平时主要会涉及的参数有如下几个：

   - **n_clusters**: 即k值，一般需要多试一些值以获得较好的聚类效果。默认为8个聚类簇
   - **max_iter**： 最大的迭代次数，一般如果是凸数据集的话可以不管这个值，如果数据集不是凸的，可能很难收敛，此时可以指定最大的迭代次数让算法可以及时退出循环。
   - **n_init：**用不同的初始化质心运行算法的次数。由于K-Means是结果受初始值影响的局部最优的迭代算法，因此需要多跑几次以选择一个较好的聚类效果，默认是10，一般不需要改。如果你的k值较大，则可以适当增大这个值。
   - **init：** 即初始值选择的方式，可以为完全随机选择'random',优化过的'k-means++'或者自己指定初始化的k个质心。一般建议使用默认的'k-means++'。
   - **algorithm**：有“auto”, “full” or “elkan”三种选择。"full"就是我们传统的K-Means算法， “elkan”是我们原理篇讲的elkan K-Means算法。默认的"auto"则会根据数据值是否是稀疏的，来决定如何选择"full"和“elkan”。一般数据是稠密的，那么就是 “elkan”，否则就是"full"。一般来说建议直接用默认的"auto"

   这里尝试默认参数：

   ```python
   Ks = range(2,8)
   dbis = []
   for i in Ks:
       kmeans_model = KMeans(n_clusters=i, random_state=1).fit(X1)
       labels = kmeans_model.labels_
       dbi = davies_bouldin_score(X1, labels)
       dbis.append(dbi)
   ```

   

3. 结果评价：

   $\rightarrow \mathrm{DB}$ 指数（Davies-Bouldin Index，简称 DBI）
   $$
   \mathrm{DBI}=\frac{1}{k} \sum_{i=1}^{k} \max _{j \neq i}\left(\frac{\operatorname{avg}\left(C_{i}\right)+\operatorname{avg}\left(C_{j}\right)}{d_{c e n}\left(\mu_{i}, \mu_{j}\right)}\right)
   $$
   $\Rightarrow$ 其中， $\mu$ 代表族 $\mathrm{C}$ 的中心点， $\operatorname{avg}(\mathrm{C})$ 对应于族 $\mathrm{C}$ 内样本间的平均距离，$d_{c e n}\left(\mu_{i}, \mu_{j}\right)$ 对应于族 $C_{i}$ 和 $C_{j}$ 中心点间的距离。显然，DBI 的值越小越好。
   
   其中sklearn中有方法可直接计算：
   
   **DBI的sklearn中的定义：**
   
   ```python
   def davies_bouldin_score(X, labels):
   '''
   X：表示要聚类的样本数据，一般形如（samples，features）的格式
   labels：即聚类之后得到的label标签，形如（samples，）的格式
   '''
   ```
   
   
   
4. 聚类可视化：

   **MDS(多维尺度变换)**

   多维尺度变换算法解决的问题是:当n个对象之间的相似性给定，确定这些对象在低维空间中的表示，并使其尽可能与原先的相似性大致匹配。高维空间中每一个点代表一个对象，因此点与点之间的距离和对象之间的相似度高度相关。可以这么理解，两个相似的对象在高维空间中由两个距离相近的点所表示，两个不相似的对象在高维空间中由两个距离比较远的点表示。

   ```python
   mds = MDS( n_components=2, metric=True)
   new_X_mds = mds.fit_transform(X1)
   plt.scatter(new_X_mds [:,0], new_X_mds [:,1], c=labels)
   plt.show()
   ```




### （4）实验结果:

1. **DataSetMeans1.csv**：

   - DBI与K的关系图：

     ![image-20210517142043867](https://i.loli.net/2021/05/17/ue7CIA8aKzy6mhn.png)

     可见，当K=3时，DBI存在最小值。

     

   - 聚类可视化：

     如下图所示：

     ![image-20210517144001201](https://i.loli.net/2021/05/17/GiwLhart8xbVMTY.png)

     

2. **DataSetMeans2.csv**：

   - DBI与K的关系图：

     ![image-20210517143614883](https://i.loli.net/2021/05/17/7NUskRWwAbBaVq5.png)

     可见，也是在K = 3时DBI最小。

     

   - 聚类可视化：

     ![image-20210517143448733](https://i.loli.net/2021/05/17/9PWnpEOdKQTF7Dm.png)

     

## 实验小结：

聚类算法时一种无监督的学习方法。所谓无监督学习是指实现不知道要寻找的内容，即没有目标变量。聚类将数据点归到多个簇中，其中相似的数据点处于同一簇，而不相似的数据点处于不同簇中。

同时，学习到关于聚类中K选取方法的经验：

- 分析数据，人工判断；
- 基于变化的算法：即定义一个函数，随着K的改变，认为在正确的K时会产生极值；
- 基于变化的算法：即定义一个函数，随着K的改变，认为在正确的K时会产生极值；
- 基于结构的算法：即比较类内距离、类间距离以确定K；
-  基于一致性矩阵的算法：即认为在正确的K时，不同次聚类的结果会更加相似；
- 基于层次聚类：即基于合并或分裂的思想，在一定情况下停止从而获得K；
- 基于采样的算法：即对样本采样，分别做聚类；根据这些结果的相似性确定K；
- 使用Canopy Method算法进行初始划分；
- 使用BIC算法进行初始划分。

