
# **《机器学习导论》实验报告**

[TOC]



## **任务一：使用朴素贝叶斯过滤垃圾邮件**

### 1.问题描述：

现有50封电子邮件，存放在数据集task1中，试基于朴素贝叶斯分类器原理，用Python编程实现对垃圾邮件和正常邮件的分类。采用交叉验证方式并且输出分类的错误率及分类错误的文档。

### 2.实验原理：

- **朴素贝叶斯：**

  基于概率论的分类方法：朴素贝叶斯，要求分类器给出一个最优的类别猜测结果，同时给出这个猜测的概率估计值。一种有效计算条件概率的方法称为贝叶斯准则，贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知$ P(x|c)$，要求 $P(c|x)$，那么可以使用下面的计算方法：
  $$
  P(c \mid x)=\frac{P(x \mid c) P(c)}{P(x)}
  $$

  于是，可以定义贝叶斯分类准则为：若那么属于类别若$ P(c1|x,y)>P(c2|x,y)$, 那么属于类别 C1

  若那么属于类别若 $P(c1|x,y)<P(c2|x,y)$, 那么属于类别 C2

  使用贝叶斯准则，可以通过已知的三个概率值来计算未知的概率值。

  

- **由词向量计算：**

  由于这里的特征是词向量，于是重写贝叶斯准则，将之前的x、y 替换为w。粗体w表示这是一个向量，即它由多个数值组成，长度N为词典长度。在词袋模型中，数值个数与词典中的词个数相同，贝叶斯准则公式如下：
  $$
  P(ci|w)=P(w|ci)P(ci)P(w)
  $$
   我们使用上述公式，对每个类别计算该值，然后比较这两个概率值的大小。首先通过类别（垃圾邮件或正常邮件）中样本数除以总的样本数来计算概率$ P(ci)$,接下里计算 $P(w|ci)$ ，这里使用朴素贝叶斯假设，若将w展开为一个个独立特征，那么就可以将上述概率写作$ P(w0,w1,w2,...wN|ci)$ ，这里假设所有词都互相独立，该假设也称作条件独立性假设，它意味着可以使用 如下公式计算上述概率：
  $$
  P(w|ci)=P(w0|ci)P(w1|ci)P(w2|ci)...P(wN|ci)
  $$
  这样就极大的简化了计算过程。

  

- 具体步骤：

1. 设D是词向量集合，每个词向量用一个N维向量 $w=\{w0,w1,w2,...,wN\} $表示。

2. 假定有m个类 $ci=\{c1,c2,...,cm\}$，给定词向量w，分类法将预测w属于最高后验概率的类，即，朴素贝叶斯分类法预测w属于类ci，当且仅当
   $$
   P(ci|w)>P(cj|w),  1≤j≤m,j≠i​
   $$
   其中，P(ci|w) 最大的类 ci 称为最大后验概率。

3. 做条件独立性假设，于是将公式简化:
   $$
   P\left(w \mid c_{i}\right)=\prod_{k=1}^{N} w_{k} \mid c_{i}
   $$
   
4. 训练：利用公式，对每个词向量每个类别分别计算条件概率，得出 $P0v,P1v,Pspam$ ，分类器训练完成。

5. 应用：利用贝叶斯分类准则，对预测词向量分别计算$ P0,P1$ ，选择最大概率对应的分类作为目标预测结果。

   

### 3.代码实现：

1. 数据处理：

   1）需要从文本中获取特征，需要拆分文本，分成词条

   2）获取数据字典

   3）向量化处理，方便进行特征提取

   实现：

   - 分词：

     ```python
     def textParse(bigString):
         import re
         listOfTokens=re.split('\W',bigString)
         #匹配非字母数字下划线
         return [tok.lower() for tok in listOfTokens if len(tok)>2] 
     	#取出掉长度过短的单词
     ```

     

   - 获取数据字典：

     ```python
     def creatVocabList(dataset):
         vocabSet=set([])
         for document in dataset:
              vocabSet=vocabSet|set(document)  #两个集合的并集
         return list(vocabSet)
     ```

     

   - 向量化处理：

     这里可以使用词集模型或者词袋模型：

     1）词集模型：只关注token是否出现，并不关注出现的次数。

     ```python
     def setOfWords2Vec(vocabList, inputSet):
       returnVec = [0]*len(vocabList)
       for word in inputSet:
       	if word in vocabList:
         	return Vec[vocabList.index(word)] = 1
       return returnVec
     ```

     2）词袋模型：关注token的出现次数，有较好的处理结果

     ```python
     #词袋模型
     def bagOfWords2Vec(vocabList,inputSet): #参数分别为词汇表，输入文档
         returnVec=[0]*len(vocabList)
         for word in inputSet:
             if word in vocabList:
                 returnVec[vocabList.index(word)]+=1
         return returnVec
     ```

     

2. 训练模型：

   朴素贝叶斯是基于概率的分类器，其训练过程就是计算各个概率的过程。

   流程为：

   ```
   对每篇训练文档：
   ===对每个类别：
   ======如果词条出现在文档中，则增加该词条的计数
   ======增加所有出现词条的计数值
   ===对每个类别：
   ======对每个词条：
   =========将该词条的数目除以总词条的数目得到条件概率
   ===返回每个类别的条件概率
   ```

   代码：

   ```python
   #朴素贝叶斯分类函数
   def classifyNB(vec2Classify,p0Vec,p1Vec,pClass): #参数分别为：要分类的向量以及使用trainNB0()计算得到的三个概率
       p1=sum(vec2Classify*p1Vec)+np.log(pClass)
       p0=sum(vec2Classify*p0Vec)+np.log(1-pClass)
       if p1>p0:
           return 1  
       else:
           return 0
   ```

3. 测试：

   在50封邮件中，正常邮件和垃圾邮件各占25，在在其中随机抽取10封电子邮件，预测其所在的类别。

   ```python
   #测试算法：使用朴素贝叶斯交叉验证。同时保存分类模型的词汇表以及三个概率值，避免判断时重复求值
   def spamTest():
       docList = []  # 文档（邮件）矩阵
       classList = []  # 类标签列表
       for i in range(1, 26):
           wordlist = textParse(open('spam/{}.txt'.format(str(i))).read())
           docList.append(wordlist)
           classList.append(1)
           wordlist = textParse(open('ham/{}.txt'.format(str(i))).read())
           docList.append(wordlist)
           classList.append(0)
       vocabList = creatVocabList(docList)  # 所有邮件内容的词汇表
       import pickle
       file=open('vocabList.txt',mode='wb')  #存储词汇表
       pickle.dump(vocabList,file)
       file.close()
       # 对需要测试的邮件，根据其词表fileWordList构造向量
       # 随机构建40训练集与10测试集
       trainingSet = list(range(50))
       testSet = []
       for i in range(10):
           randIndex = int(np.random.uniform(0, len(trainingSet)))
           testSet.append(trainingSet[randIndex])
           del (trainingSet[randIndex])
       trainMat = []  # 训练集
       trainClasses = []  # 训练集中向量的类标签列表
       for docIndex in trainingSet:
           # 使用词袋模式构造的向量组成训练集
           trainMat.append(bagOfWords2Vec(vocabList, docList[docIndex]))
           trainClasses.append(classList[docIndex])
       p0v,p1v,pAb=trainNB0(trainMat,trainClasses)
       file=open('threeRate.txt',mode='wb') #用以存储分类器的三个概率
       pickle.dump([p0v,p1v,pAb],file)
       file.close()
       errorCount=0
       for docIndex in testSet:
           wordVector=bagOfWords2Vec(vocabList,docList[docIndex])
           if classifyNB(wordVector,p0v,p1v,pAb)!=classList[docIndex]:
               errorCount+=1
       return float(errorCount)/len(testSet)
   ```

### 4.测试结果：

由于测试集的生成是随机的，所以分类器误分率每次运行结果不一致。通过运行10次的平均值作为分类器的分类效果：

![分类效果](C:\Users\Yuetian\AppData\Roaming\Typora\typora-user-images\image-20210421141951983.png)





## **任务二：使用朴素贝叶斯对搜狗新闻语料库进行分类**

### 1.数据集讲解：

数据集存放在task2/Database中，新闻一共分为9个类：财经、IT、健康、体育、旅游、教育、招聘、文化、军事，不同种类的新闻分别放在相应代号的文件夹中。NBC.py中，对新闻进行预处理的模块已经给出，请尝试补全分类器模块的代码（NBC.py中函数TextClassifier），自行划分训练集和测试集，对这些新闻进行训练和分类。

### 2.编程思路：

1. 数据载入与清洗
2. 使用jieba分词、停用词处理
3. 随机抽取训练和测试样本
4. 文本特征提取（词向量）
5. 朴素贝叶斯分类（采用多项式模型）

### 3.具体实现：

其余模块已给出，朴素贝叶斯分类使用多项式模型，模型如下：

**多项式贝叶斯分类器（MultinomialNB）**

> sklearn.naive_bayes.MutlnomialNB

- 参数
  - alpha：浮点数，指定朴素贝叶斯估计公式中λ值
  - fit_prior=True：是否学习P(y=Ck)，不学习则以均匀分布替代
  - class_prior=None：可以传入数组指定每个分类的先验概率，None代表从数据集中学习先验概率
  - class_count：数组，形状为(n_class,)，每个类别包含训练样本数量
  - feature_count：数组，形状(n_class,n_features)，每个类别每个特征遇到的样本数

先使用默认参数尝试：

```python
def TextClassifier(train_feature_list, test_feature_list, train_class_list, test_class_list):
    """
    函数说明:分类器
    Parameters:
        train_feature_list - 训练集向量化的特征文本
        test_feature_list - 测试集向量化的特征文本
        train_class_list - 训练集分类标签
        test_class_list - 测试集分类标签
    Returns:
        test_accuracy - 分类器精度
    """
    classifier = MultinomialNB()
    # 模型训练
    classifier.fit(train_feature_list, train_class_list)
    # 使用训练好的模型进行预测
    test_accuracy = classifier.score(test_feature_list, test_class_list)
    return test_accuracy
```



### 4.结果展示

1. **默认参数下的运行结果：**

   ![image-20210421211012357](C:\Users\Yuetian\AppData\Roaming\Typora\typora-user-images\image-20210421211012357.png)

   

2. **删去不同个数的高频词：**

   ```python
   deleteNs = range(0, 1000, 20)  # 0 20 40 60 ... 980
   for deleteN in deleteNs:
       feature_words = words_dict(all_words_list, deleteN, stopwords_set)
       train_feature_list, test_feature_list = TextFeatures(train_data_list, test_data_list, feature_words)
       test_accuracy = TextClassifier(train_feature_list, test_feature_list, train_class_list, test_class_list)
       test_accuracy_list.append(test_accuracy)
   plt.figure()
   plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
   plt.plot(deleteNs, test_accuracy_list)
   plt.title('Relationship of deleteNs and test_accuracy')
   plt.xlabel('deleteNs')
   plt.ylabel('test_accuracy')
   plt.show()
   ```

   通过绘制出了deleteNs和test_accuracy的关系，以此大致确定去掉前多少的高频词汇效果最好。

   但是每次运行程序，绘制的图形差别还是比较大，感觉原因应该是因为每次划分数据集时有随机性，但是因为数据集的规模太小，所以不同划分对结果的影响较大。

   但是我们可以通过多次测试，来决定这个deleteN的取值，然后确定这个参数，下面是几次运行的结果：

   第一次：

   ![image-20210421195548737](C:\Users\Yuetian\AppData\Roaming\Typora\typora-user-images\image-20210421195548737.png)

   

   第二次：

   ![image-20210421195706447](C:\Users\Yuetian\AppData\Roaming\Typora\typora-user-images\image-20210421195706447.png)

   

   第三次：

   ![image-20210421195823498](C:\Users\Yuetian\AppData\Roaming\Typora\typora-user-images\image-20210421195823498.png)

   观察可得，deleteN每次在375左右有着不错的效果，选取deleteN为375.

   

3. **改变特征词数量：**

   更改不同的特征词数量，观察对分类准确率的影响：

   ```python
       test_accuracy_list = []
       words_lens = range(400, 1000, 50)
       for words_len in words_lens:
           feature_words_t = feature_words[:words_len]
           train_feature_list, test_feature_list = TextFeatures(train_data_list, test_data_list, feature_words_t)
           test_accuracy = TextClassifier(train_feature_list, test_feature_list, train_class_list, test_class_list)
           test_accuracy_list.append(test_accuracy)
       plt.figure()
       plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
       plt.plot(words_lens, test_accuracy_list)
       plt.title('特征词数量与test_accuracy关系')
       plt.xlabel('特征词数量')
       plt.ylabel('test_accuracy')
       plt.show()
   ```

   结果：

   ![image-20210421214247552](C:\Users\Yuetian\AppData\Roaming\Typora\typora-user-images\image-20210421214247552.png)

   

4. **更改训练集和测试集划分比例：**

   更改训练集和测试集划分比例，观察不同训练集规模对实验结果的影响:

   ```python
   sizes = range(5,50)
   size_n =[size/100 for size in sizes]
   for test_s in size_n:
       all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = TextProcessing(folder_path, test_size = test_s)
       # 文本特征提取和分类
       deleteN = 450
       feature_words = words_dict(all_words_list, deleteN, stopwords_set)
       train_feature_list, test_feature_list = TextFeatures(train_data_list, test_data_list, feature_words)
       test_accuracy = TextClassifier(train_feature_list, test_feature_list, train_class_list, test_class_list)
       test_accuracy_list.append(test_accuracy)
   plt.figure()
   plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
   plt.plot(size_n, test_accuracy_list)
   plt.title('Relationship of test_size and test_accuracy')
   plt.xlabel('测试集比例')
   plt.ylabel('test_accuracy')
   plt.show()
   ```

   结果：

   ![image-20210421202947728](C:\Users\Yuetian\AppData\Roaming\Typora\typora-user-images\image-20210421202947728.png)

   

   可见随着训练集的减小成下降趋势。

   

5. **切换使用伯努利模型：**

   伯努利模型用于多重伯努利分布的数据，即有多个特征，但每个特征都假设是一个二元 (Bernoulli, boolean) 变量。特征取值只能是0和1(如文本分类中某个词出现，特征为1;无，特征为0)应该不适用与该场景，验证如下：

   ![image-20210421204844825](C:\Users\Yuetian\AppData\Roaming\Typora\typora-user-images\image-20210421204844825.png)

   

6. **使用TF-IDF：**

   ```python
   from sklearn.feature_extraction.text import TfidfVectorizer
   transfomer = TfidfVectorizer()
   tf_train_data = []
   for tx in train_data_list:
       tf_train_data.append(transfomer.fit_transform(tx))
   tf_test_data = []
   for tx1 in test_data_list:
       tf_test_data.append(transfomer.transform(tx1))
   ```

   结果

   ![image-20210421230611311](C:\Users\Yuetian\AppData\Roaming\Typora\typora-user-images\image-20210421230611311.png)

   由此可知，特征量越多，多项式优势越明显

   

### 5.思考

1. 在训练朴素贝叶斯分类器之前，要处理好训练集，文本的清洗十分重要。

2. 根据提取的分类特征将文本向量化，然后训练朴素贝叶斯分类器。

3. 去高频词汇数量的不同，对结果也是有较大影响。

4. 拉普拉斯平滑对于改善朴素贝叶斯分类器的分类效果有着积极的作用。



## **任务三：使用朴素贝叶斯对电影评论分类**

### 1.数据集讲解：

​    该数据集是IMDB电影数据集的一个子集，已经划分好了测试集和训练集，训练集包括25000条电影评论，测试集也有25000条，该数据集已经经过预处理，将每条评论的具体单词序列转化为词库里的整数序列，其中每个整数代表该单词在词库里的位置。例如，整数104代表该单词是词库的第104个单词。为实验简单，词库仅仅保留了10000个最常出现的单词，低频词汇被舍弃。每条评论都具有一个标签，0表示为负面评论，1表示为正面评论。

​    训练数据在train_data.txt文件下，每一行为一条评论，训练集标签在train_labels.txt文件下，每一行为一条评论的标签；测试数据在test_data.txt文件下，测试数据标签未给出。

### 2.具体实现：

1. **取出数据集：**

   从txt中取出训练集与测试集：

   ```python
   with open("test/test_data.txt", "rb") as fr:
   
     test_data_n = [inst.decode().strip().split(' ') for inst in fr.readlines()]
   
     test_data = [[int(element) for element in line] for line in test_data_n]
   
   test_data = np.array(test_data)
   ```

   

 

2.  **数据处理：**

   对每条评论，先将其解码为英文单词，再键值颠倒，将整数索引映射为单词。

   把整数序列编码为二进制序列。

   最后把训练集标签向量化。

   ```python
   # 将某条评论解码为英文单词
   
   word_index = imdb.get_word_index() # word_index是一个将单词映射为整数索引的字典
   
   reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
   
   # 键值颠倒，将整数索引映射为单词
   
   decode_review = ' '.join(
   
     [reverse_word_index.get(i - 3, '?') for i in train_data[0]]
   
   ) 
   
   \# 将评论解码
   
   \# 注意，索引减去了3，因为0,1,2是为padding填充
   
   \# "start sequence"序列开始，"unknow"未知词分别保留的索引
   
    
   
   \# 将整数序列编码为二进制矩阵
   
   def vectorize_sequences(sequences, dimension=10000):
   
     results = np.zeros((len(sequences), dimension)) # 创建一个形状为(len(sequences), dimension)的矩阵
   
     for i, sequence in enumerate(sequences):
   
       results[i, sequence] = 1 # 将results[i]的指定索引设为 1
   
     return results
   
   x_train = vectorize_sequences(train_data)
   
   x_test = vectorize_sequences(test_data)
   
   \# 标签向量化
   
   y_train = np.asarray(train_labels).astype('float32')
   ```

3. **建立模型：**

   可选多项式模型或者伯努利模型。

   二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。
   计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。
   当训练集文档较短，也就说不太会出现很多重复词的时候，多项式和伯努利模型公式的分子相等，多项式分母值大于伯努利分子值，因此多项式的似然估计值会小于伯努利的似然估计值。
   所以，当训练集文本较短时，我们更倾向于使用伯努利模型。而文本较长时，我们更倾向于多项式模型，因为，在一篇文档中的高频词，会使该词的似然概率值相对较大。

   使用拉普拉斯平滑

   `alpha`:先验平滑因子，默认等于1，当等于1时表示拉普拉斯平滑。

   ```python
   # model = MultinomialNB()
   model = BernoulliNB()
   model.fit(X_train, y_train)
   ```

4.   **输出测试集上的预测结果：**

   将结果写入txt

   ```python
   # model evaluation
   print("model accuracy is " + str(accuracy_score(y_test, y_pred)))
   print("model precision is " + str(precision_score(y_test, y_pred, average='macro')))
   print("model recall is " + str(recall_score(y_test, y_pred, average='macro')))
   print("model f1_score is " + str(f1_score(y_test, y_pred, average='macro')))
   
   des = y_pred_local.astype(int)
   np.savetxt('Text3_result.txt', des, fmt='%d', delimiter='\n')
   ```

### 3.实验结果：

使用多项式模型：

![image-20210421233103464](C:\Users\Yuetian\AppData\Roaming\Typora\typora-user-images\image-20210421233103464.png)

使用伯努利模型：

![image-20210421233455235](C:\Users\Yuetian\AppData\Roaming\Typora\typora-user-images\image-20210421233455235.png)

在该场景下，两者差别不大。




## 实验总结

1. 贝叶斯概率及贝叶斯 准则提供了一种利用已知值来估计位置概率的有效方法；
2. 朴素贝叶斯假设数据特征之间相互独立，虽然该假设在一般情况下并不严格成立，但使用朴素贝叶斯进行分类，仍然可以取得很好的效果；
3. 贝叶斯网络的优点：在数据较少的情况下仍然有效，可以处理多类别问题；
4. 贝叶斯网络的缺点：对输入数据的准备方式较为敏感。
5. 拉普拉斯平滑对于改善朴素贝叶斯分类器的分类效果有着积极的作用。