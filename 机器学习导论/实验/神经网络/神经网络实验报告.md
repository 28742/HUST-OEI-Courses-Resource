

# **《机器学习导论》实验报告**

[TOC]

------



## 任务一：使用Logistic回归估计马疝病的死亡率

### （1）问题描述：

**训练集：**包含于文件horseColicTraining.txt中，用于训练得到模型的最佳系数。训练集包含299个样本（299行），每个样本含有21个特征（前21列），这些特征包含医院检测马疝病的指标；最后1列为类别标签，表示病马的死亡情况；部分样本含有缺失值。

**测试集：**包含于文件horseColicTest.txt中，通过预测测试样本中病马的死亡情况，来评估训练模型的优劣。测试集包含69个样本，部分样本部分特征缺失；最后1列为类别标签，用于计算错误率。

训练集与测试集存储形式如下：

![存储形式](https://i.loli.net/2021/05/11/QKOGyrhJklptVd7.png)



### （2）模型原理：

logistic回归是一种广义线性回归（generalized linear model），因此与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有 $ wx+b $ ，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将w‘x+b作为因变量，即 $y =wx+b$ ，而logistic回归则通过函数L将 $wx+b$ 对应一个隐状态 $p =L(wx+b)$ ，然后根据p 与1-p的大小决定因变量的值。如果L是logistic函数，就是logistic回归。




### （3）具体实现：

1. **准备数据：**

   对于本次实验所用的数据集，需要进行如下预处理：

   - 所有的缺失值必须用一个实数值来替换，这里选择实数0来替换所有缺失值，恰好能适用于Logistic回归，这样做在更新时不会影响回归系数的值。另外由于sigmoid(0)=0.5，即它对结果的预测不具有任何倾向性，因此上述做法也不会对误差造成任何影响。

   - 测试数据集中发现一条数据的类别标签已经缺失，那么应将这条数据丢弃，这是因为类别标签与特征不同，很难确定采用某个合适的值来替换.

   这里的 'horseColicTest.txt' 和 'horseColicTraining.txt' 为已经处理好的数据。

   

2. **加载数据：**

   ```python
   '''
   数据加载
   '''
   def loadDataSet(filePath):
       f = open(filePath)
       dataList = []
       labelList = []
       for line in f.readlines():
           currLine = line.strip().split('	')
           lineArr = []
           for i in range(21):
               lineArr.append(float(currLine[i]))
           dataList.append(lineArr)
           labelList.append(float(currLine[21]))
       return dataList, labelList
   ```

   

3. **算法训练：**

   使用网络上改进后的随机梯度下降算法，修正系数：

   改进内容为调整过程中步长的动态调整。

   ```python
   '''
   sigmoid函数
   '''
   def sigmoid(inX):
       return 1.0 / (1 + np.exp(-inX))
    
    
   '''
   随机梯度下降算法
   '''
   def stoGradAscent1(dataMatrix, classLabels, numIter = 150):
       m,n = shape(dataMatrix)
       weights = ones(n)
       for j in range (numIter):
           dataIndex = range(m)
           for i in range(m):
               alpha = 4 / (1.0 + j + i) + 0.01
               randIndex = int(random.uniform(0, len(dataIndex)))
               h = sigmoid(sum(dataMatrix[randIndex] * weights))
               error = classLabels[randIndex] - h
               weights = weights + alpha * error * dataMatrix[randIndex]
               del (list(dataIndex)[randIndex])
       return weights
   '''
   随机梯度下降算法
   '''
   def stocGradAscent0(dataList, labelList):
       dataArr = np.array(dataList)  # 数据转化为矩阵
       m, n = np.shape(dataArr)
       alpha = 0.01
       weights = np.ones(n)
       for i in range(m):
           h = sigmoid(np.sum(dataArr[i]*weights))
           error = (h-labelList[i])
           weights = weights-alpha*error*dataArr[i]
       return weights
   
   '''
   改进的随机梯度下降算法
   '''
   def stocGradAscent1(dataList, labelList, numIter=150):
       dataArr = np.array(dataList)
       m,n = np.shape(dataArr)
       weights = np.ones(n)
       for j in range(numIter):
           dataIndex = list(range(m))
           for i in range(m):
               alpha = 4/(1.0+j+i)+0.01        # 步长为动态变化
               rand = int(np.random.uniform(0, len(dataIndex)))
               choseIndex = dataIndex[rand]
               h = sigmoid(np.sum(dataArr[choseIndex]*weights))
               error = h-labelList[choseIndex]
               weights = weights-alpha*error*dataArr[choseIndex]
               del(dataIndex[rand])
       return weights
   
   ```

   

4. **使用模型分类：**

   该函数以回归系数和特征向量作为输入来计算对应的Sigmoid值，如果Sigmoid值大于0.5则函数返回1。

   ```python
   '''
   进行分类
   '''
   def classifyVector(inX, weights):
       prob = sigmoid(np.sum(inX * weights))
       if prob > 0.5:
           return 1.0
       else:
           return 0.0
       
   ```


5. **算法评估：**

   在测试集上计算分类精度，多次运行得到的结果可能稍有不同，主要是因为其中有随机的成分在里面。只有当梯度下降算法得到的回归系数已经完全收敛，那么结果才是确定的。

   ```python
   '''
   在测试集上计算分类精度
   '''
   def colicTest(trainWeights, testDataList, testLabelList):
       rightCount = 0  # 判断错误的数量
       testCount = len(testDataList)
       for i in range(testCount):
           if int(classifyVector(np.array(testDataList[i]), trainWeights))==int(testLabelList[i]):
               rightCount += 1
       acc = float(rightCount)/testCount
       print("本次的精度为%f" % acc)
       return acc
   
   
   def main():
       numTests = 10
       accSum = 0.0
       trainDataList, trainLabelList = loadDataSet("horseColicTraining.txt")
       testDataList, testLabelList = loadDataSet("horseColicTest.txt")
       for i in range(numTests):
           trainWeights = stocGradAscent(trainDataList, trainLabelList, 500)
           errorSum += colicTest(trainWeights, testDataList, testLabelList)
       print("这%d次的平均精度为%f"%(numTests, accSum/numTests))
   ```

   

### （4）实验结果：

运行结果如下图所示：

![运行结果](https://i.loli.net/2021/05/11/vwFqt1kPJEu9dfD.png)



### （5）使用Sklearn构建LR分类器

代码如下所示：

```python
import numpy as np
from sklearn.linear_model import LogisticRegression

def colicSklearn():
    #使用np读取数据
    trainFiled=np.loadtxt('horseColicTraining.txt',delimiter="\t")
    trainSet = trainFiled[:,:-1]
    trainLables=trainFiled[:,-1:]

    testFiled = np.loadtxt('horseColicTest.txt', delimiter="\t")
    testSet = testFiled[:, :-1]
    testLables = testFiled[:, -1:]
    classifier=LogisticRegression(solver='liblinear',max_iter=10).fit(trainSet,trainLables)
    test_accurcy=classifier.score(testSet,testLables) *100

    print('正确率：%f%%'%test_accurcy)
if __name__ == '__main__':
    colicSklearn()
```

运行结果：

![image-20210429165015294](https://i.loli.net/2021/05/11/dVxuH6fUCF79rPR.png)

与之前结果相比有一定提升。



### （6）实验小结：

Logistic 回归的目的是寻找一个非线性的函数Sigmoid最佳拟合参数，求解过程可以由最优化算法来实现。在最优化算法中，**最常用的是梯度上升算法**，而梯度上升算法又可以简化为随机梯度上升算法。

随机梯度上升算法与梯度上升算法的效果相当，但是占用更少的计算资源；此外随机梯度上升是一个在线算法，他可以在新数据到来时就程程参数的更新，而不需要重新读取整个数据集来进行批处理计算。

**Logistic回归的优缺点**
优点：实现简单，易于理解和实现；计算代价不高，速度很快，存储资源低。
缺点：容易欠拟合，分类精度可能不高。





------

## 任务二：使用神经网络完成新闻分类

### （1）问题描述：

该数据集用于文本分类，包括大约20000个左右的新闻文档，均匀分为20个不同主题的新闻组集合，其中：

**训练集：**包括11314个新闻文档及其主题分类标签。训练数据在文件train目录下，训练新闻文档在train_texts.dat文件中，训练新闻文档标签在train_labels.txt文档中，编号为0~19，表示该文档分属的主题标号。

**测试集：**包括7532个新闻文档，标签并未给出。测试集文件在test目录下，测试集新闻文档在test_texts.dat文件中。

### （2）模型原理： 

**LSTM**

长短期记忆网络（[LSTM](https://baike.baidu.com/item/LSTM/17541102)，Long Short-Term Memory）是一种时间循环神经网络，是为了解决一般的[RNN](https://baike.baidu.com/item/RNN/5707183)（[循环神经网络](https://baike.baidu.com/item/循环神经网络/23199490)）存在的长期依赖问题而专门设计出来的，所有的RNN都具有一种重复神经网络模块的链式形式。在标准RNN中，这个重复的结构模块只有一个非常简单的结构，例如一个tanh层。

**参考：**[Tensorflow实战：LSTM原理及实现（详解）_m0_37917271的博客-CSDN博客](https://blog.csdn.net/m0_37917271/article/details/82350571)

做以下简要总结：

如下图所示：

![img](https://upload-images.jianshu.io/upload_images/6592751-dbad8a300a2adc52.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

从图中可以看到，LSTM 提出了三个门（gate）的概念：input gate，forget gate，output gate。其实可以这样来理解，input gate 决定了对输入的数据做哪些处理，forget gate 决定了哪些知识被过滤掉，无需再继续传递，而 output gate 决定了哪些知识需要传递到下一个时间序列。

计算依据下列公式：
$$
\begin{array}{c}
i_{t}=\sigma\left(W_{i i} x_{t}+b_{i i}+W_{h i} h_{(t-1)}+b_{h i}\right) \\
f_{t}=\sigma\left(W_{i f} x_{t}+b_{i f}+W_{h f} h_{(t-1)}+b_{h f}\right) \\
g_{t}=\tanh \left(W_{i g} x_{t}+b_{i g}+W_{h g} h_{(t-1)}+b_{h g}\right) \\
o_{t}=\sigma\left(W_{i o} x_{t}+b_{i o}+W_{h o} h_{(t-1)}+b_{h o}\right) \\
c_{t}=f_{t} * c_{(t-1)}+i_{t} * g_{t} \\
h_{t}=o_{t} * \tanh \left(c_{t}\right)
\end{array}
$$
其中：

-  $i_{t}$ 是处理 input 的 input gate, 外面一个 sigmoid 函数处理，其中的输入是当前输入 $x_{t}$ 和前一个 时间状态的输出 $h_{(t-1)}$ 。所有的 $b$ 都是偏置项。
- $f_{t}$ 则是 forget gate 的操作，同样对当前输入 $x_{t}$ 和前一个状态的输出 $h_{(t-1)}$ 进行处理。
- $g_{t}$ 也是 input gate 中的操作, 同样的输入，只是外面换成了 $\tanh$ 函数。
- $o_{t}$ 是前面图中 output gate 中左下角的操作，操作方式和前面 $i_{t}$, 以及 $f_{t}$ 一样。
- $c_{t}$ 则是输出之一, 对 forget gate 的输出, input gate 的输出进行相加，然后作为当前时间序列的 一个隐状态向下传递。
- $h_{t}$ 同样是输出之一，对 前面的 $c_{t}$ 做一个 $\mathrm{tanh}$ 操作，然后和前面得到的 $o_{t}$ 进行相乘, $h_{t}$ 既向 下一个状态传递，也作为当前状态的输出。





### （3）实现过程：

1. 读入数据集：

   读取文件train_texts.dat和test_texts.dat方式如下，以train_texts.dat为例，test_texts.dat读取方式相同；标签文件为正常txt文件，读取方式按照读取txt文件即可。

   ```python
   # 读入数据
   file_name = 'train/train_texts.dat'
   with open(file_name, 'rb') as f:
       train_texts = pickle.load(f)
   file_name = 'test/test_texts.dat'
   with open(file_name, 'rb') as f:
       test_texts = pickle.load(f)
   
   train_labals = []
   fl = open('train/train_labels.txt')
   for line in fl.readlines():
       train_labals.append(line)
   ```

   

2. 特征提取：

   因为每篇新闻都是由英文字符表示而成，因此需要首先提取每篇文档的特征，把每篇文档抽取为特征向量，这里我们选择提取文档的TF-IDF特征，即词频（TF）-逆文本频率（IDF）。

   提取文档的TF-IDF特征可以通过sklearn. feature_extraction.text中的TfidfVectorizer来完成，具体实现代码如下：

   ```python
   # TFIDF向量化
   vectorizer = TfidfVectorizer(max_features=10000)
   train_vector = vectorizer.fit_transform(train_texts)
   print(train_vector.shape)
   test_vector = vectorizer.transform(test_texts)
   print(test_vector.shape)
   ```

   

3. 标签向量化：

   将标签向量化有两种方法：可以将标签列表转换为整数张量，或者使用one-hot 编码。 

   one-hot 编码是分类数据广泛使用的一种格式，也叫分类编码（categorical encoding）。这里我们采用该种方法，标签的one-hot编码就是将每个标签表示为全零向量， 只有标签索引对应的元素为 1。代码实现如下：

   ```python
   from keras.utils import to_categorical
   one_hot_train_labels = to_categorical(train_labals)
   ```

   

4. 拆分训练集和测试集：

   测试集比例选择为0.2：

   ```python
   #拆分测试集与训练集
   X_train, X_test, y_train, y_test = train_test_split(train_vector, one_hot_train_labels, test_size=0.2, random_state=0)
   x_test = X_test.toarray()
   partial_x_train = X_train.toarray()
   ```

   

5. 构建网络：

   这里先尝试最简单的3个全连接层的网络。

   输出类别的数量为20个。输出空间的维度较大。 对于用过的 Dense 层的堆叠，每层只能访问上一层输出的信息。如果某一层丢失了与 分类问题相关的一些信息，那么这些信息无法被后面的层找回，也就是说，每一层都可能成为信息瓶颈。因此，设计网络如下所示：

   ```python
   # 定义Sequential类
   model = models.Sequential()
   # 全连接层，128个节点
   model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))
   # 全连接层，64个节点
   model.add(layers.Dense(64, activation='relu'))
   # 全连接层，得到输出
   model.add(layers.Dense(20, activation='softmax'))
   # loss
   model.compile(optimizer='rmsprop',
                 loss='categorical_crossentropy',
                 metrics=['accuracy'])
   ```

   网络的最后一层是大小为20 的 Dense 层。这意味着，对于每个输入样本，网络都会输出一个 20 维向量。这个向量的每个元素（即每个维度）代表不同的输出类别。

   

   我们选择的损失函数是 categorical_crossentropy（分类交叉熵）。它用于衡量两个概率分布之间的距离，这里两个概率分布分别是网络输出的概率分布和标签的真实分 布。通过将这两个分布的距离最小化，训练网络可使输出结果尽可能接近真实标签。

   

6. 验证：

   现在开始训练网络，共 20 个轮次。

   ```python
   history = model.fit(partial_x_train,
                       y_train,
                       epochs=20,
                       batch_size=512,
                       validation_data=(x_test, y_test))
   ```

   截取第20次的结果：

   ```python
   Epoch 20/20
    512/9051 [>.............................] - ETA: 0s - loss: 0.0032 - accuracy: 0.9980
   1536/9051 [====>.........................] - ETA: 0s - loss: 0.0022 - accuracy: 0.9993
   2560/9051 [=======>......................] - ETA: 0s - loss: 0.0028 - accuracy: 0.9992
   3072/9051 [=========>....................] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993
   4096/9051 [============>.................] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995
   5120/9051 [===============>..............] - ETA: 0s - loss: 0.0024 - accuracy: 0.9992
   6656/9051 [=====================>........] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994
   7680/9051 [========================>.....] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995
   8704/9051 [===========================>..] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995
   9051/9051 [==============================] - 1s 79us/sample - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.3894 - val_accuracy: 0.8975
   ```

   

7. loss曲线：

   绘制loss曲线：

   ```python
   loss = history.history['loss']
   val_loss = history.history['val_loss']
   epochs = range(1, len(loss) + 1)
   plt.plot(epochs, loss, 'bo', label='Training loss')
   plt.plot(epochs, val_loss, 'b', label='Validation loss')
   plt.title('Training and validation loss')
   plt.xlabel('Epochs')
   plt.ylabel('Loss')
   plt.legend()
   plt.show()
   ```

   ![loss](https://i.loli.net/2021/05/11/k7lS3sLBUIMa4mV.png)

   

8. 精度曲线：

   绘制acc曲线：

   ```python
   plt.clf()
   acc = history.history['accuracy']
   val_acc = history.history['val_accuracy']
   plt.plot(epochs, acc, 'bo', label='Training acc')
   plt.plot(epochs, val_acc, 'b', label='Validation acc')
   plt.title('Training and validation accuracy')
   plt.xlabel('Epochs')
   plt.ylabel('Accuracy')
   plt.legend()
   plt.show()
   ```

   ![acc](https://i.loli.net/2021/05/11/YnCE36jrcTvAZdO.png)

   

9. 重新训练：

   分析可知，网络在训练 10 轮左右开始出现过拟合趋势。

   因此，我们重新训练一个新网络，共 10 个轮次，然后在测试集上评估模型。

   ```python
   history = model.fit(partial_x_train,
                       y_train,
                       epochs=10,
                       batch_size=512,
                       validation_data=(x_test, y_test))
   
   results = model.evaluate(x_test, y_test)
   ```

   ![res](https://i.loli.net/2021/05/11/gJcFNyHi6IWptQn.png)

   

10. 结果输出：

    将结果写入txt：

    ```python
    x_input = test_vector.toarray()
    predictions = model.predict(x_input)
    out_put = np.argmax(predictions,axis=1)
    np.savetxt('result.txt', out_put, fmt='%d', delimiter='\n')
    ```

    



### （4）实验小结：

1. 如果要对 N 个类别的数据点进行分类，网络的最后一层应该是大小为 N 的 Dense 层。对于单标签、多分类问题，网络的最后一层应该使用 softmax 激活，这样可以输出在 N个输出类别上的概率分布。

2. 损失函数使用分类交叉熵，它可以将网络输出的概率分布与目标的真实分布之间的距离最小化。

3. 处理多分类问题的标签有比较经典的有两种方法。
   - 通过分类编码（也叫 one-hot 编码）对标签进行编码，然后使用 categorical_crossentropy 作为损失函数。
   - 标签编码为整数，然后使用 sparse_categorical_crossentropy 损失函数。

